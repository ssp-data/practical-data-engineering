resources:
  boto3:
    config:
      aws_access_key_id: MINIO_ACCESS_KEY
      aws_secret_access_key: MINIO_SECRET_KEY
      endpoint_url: http://127.0.0.1:9000
  # s3:
  #   config:
  #     endpoint_url: http://127.0.0.1:9000 #might be needed to be 127.0.0.1:60599 or similar...
  # file_cache:
  #   config:
  #     overwrite: false
  #     target_folder: "/tmp/dagster/file_cache"
  #pyspark:
  #  config:
  #    spark_conf:
  #      spark:
  #        databricks:
  #          delta:
  #            schema:
  #              autoMerge:
  #                enabled: true #enables schemaEvolution and mergeSchema in MERGE Statement
  #        # master: "k8s://https://kubernetes.docker.internal:6443"
  #        # serviceAccount: spark
  #        delta:
  #          logStore:
  #            class: org.apache.spark.sql.delta.storage.S3SingleDriverLogStore #this is used for delta-lake
  #        executor:
  #          instances: 1
  #          memory: 4g
  #        hadoop:
  #          fs:
  #            s3a:
  #              endpoint:
  #                env: "127.0.0.1:9000"
  #              access:
  #                key:
  #                  # env: MINIO_ACCESS_KEY
  #                  env: MINIO_ROOT_USER
  #              secret:
  #                key:
  #                  # env: MINIO_SECRET_KEY
  #                  env: MINIO_ROOT_PASSWORD
  #              impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
  #              path:
  #                style:
  #                  access: "true"
  #              connection:
  #                ssl:
  #                  enabled: "false"
  #              aws:
  #                credentials:
  #                  provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
  #          delta:
  #            enableFastS3AListFrom: true
  #        sql:
  #          extensions: io.delta.sql.DeltaSparkSessionExtension #this is used for delta-lake
  #          catalog:
  #              spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
  #          # execution:
  #          #   arrow:
  #          #     pyspark:
  #          #       enabled: True #Enable Arrow-based columnar data transfers. Used for simulating SA in SA_simulation_input_nps_categories()
  #        jars:
  #          packages: io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4
  #          # packages: "io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4,org.apache.hadoop:hadoop-common:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.563"
  #          # packages: "io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.563"
  #          # repositories: "$SPARK_HOME/jars/delta-core_2.12-0.7.0.jar,$SPARK_HOME/jars/hadoop-aws-3.2.0.jar"
  #          #use "repositories" if you manually download jars and copy them to $SPARK_HOME/jars (e.g. if proxy doesn't allow)
  #          #use "packages" if you have internet connection, this way spark will automatically download dependencies with maven
  #        kubernetes:
  #          # namespace: default
  #          # authenticate:
  #          #   driver:
  #          #     serviceAccountName: spark #default
  #          # container:
  #          #   image: "spark:spark-docker"
  #          # pyspark:
  #          #   pythonVersion: 3
  #          executor:
  #            request:
  #              cores: "1"
  #            limit:
  #              cores: "4"
  #        # submit:
  #        #   deployMode: "client"
