{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:9000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import socket\n",
    "\n",
    "import sys,uuid,datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "key = os.environ[\"MINIO_ROOT_USER\"]\n",
    "secret = os.environ[\"MINIO_ROOT_PASSWORD\"]\n",
    "endpoint = os.environ[\"MINIO_SECRET_ENDPOINT\"]\n",
    "endpoint = \"http://127.0.0.1:9000\"\n",
    "print(endpoint)\n",
    "\n",
    "#sc.stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"k8s://https://kubernetes.docker.internal:6443\") \\\n",
    ".appName(\"playing_with_immo24\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.access.key\", key) \\\n",
    ".config(\"spark.hadoop.fs.s3a.secret.key\", secret) \\\n",
    ".config(\"spark.hadoop.fs.s3a.endpoint\", endpoint) \\\n",
    ".config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    ".config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0,org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
    ".config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    ".config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    ".config('spark.submit.deployMode', 'client') \\\n",
    ".config(\"spark.kubernetes.container.image\", \"spark:spark-docker\") \\\n",
    ".config(\"spark.kubernetes.pyspark.pythonVersion\", \"3\") \\\n",
    ".config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"default\") \\\n",
    ".config(\"spark.executor.instances\", \"1\") \\\n",
    ".config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    ".config(\"spark.kubernetes.executor.request.cores\",\"0.5\") \\\n",
    ".config(\"spark.kubernetes.executor.limit.cores\",\"1\") \\\n",
    ".config(\"jupyterService.jupyterPort_create_prop\", \"30888\") \\\n",
    ".config(\"serviceAccount\", \"spark\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "#.config(\"spark.driver.host\", \"10.1.2.104\") \\\n",
    "#.config(\"spark.driver.port\", \"4040\") \\\n",
    "\n",
    "sc = spark.sparkContext\n",
    "#sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_BE = \"s3a://real-estate/staging/201031_Bern_buy_0_flat.gz\" \n",
    "df_props = spark.read.json(path_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StructType\n",
    "from pyspark.sql.functions import col, explode_outer\n",
    "\n",
    "#Flatten array of structs and structs\n",
    "def flatten(df):\n",
    "\n",
    "   # compute Complex Fields (Lists and Structs) in Schema   \n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if (type(field.dataType) == ArrayType or type(field.dataType) == StructType) and field.name.startswith('propertyDetails')])\n",
    "   \n",
    "   #print(complex_fields) \n",
    "   while len(complex_fields)!=0:    \n",
    "        \n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      #print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "       \n",
    "      if col_name in [\"propertyDetails_images\",\"propertyDetails_pdfs\",\"propertyDetails_commuteTimes_defaultPois_transportations\"]:\n",
    "            #remove and skip next part\n",
    "            df=df.drop(col_name)\n",
    "      else:\n",
    "          # if StructType then convert all sub element to columns.\n",
    "          # i.e. flatten structs\n",
    "          if (type(complex_fields[col_name]) == StructType):\n",
    "             expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "             df=df.select(\"*\", *expanded).drop(col_name)\n",
    "\n",
    "\n",
    "          # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "          # i.e. explode Arrays\n",
    "          elif (type(complex_fields[col_name]) == ArrayType):\n",
    "             df=df.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "      # recompute remaining Complex Fields in Schema       \n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "      #print(complex_fields)\n",
    "      #print(df.count())\n",
    "\n",
    "   return df\n",
    "\n",
    "df_props_flatten=flatten(df_props)\n",
    "df_props_flatten.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = \"s3a://real-estate/lake/bronze/property\"\n",
    "delta_table_name='property'\n",
    "database = 'immo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_props_flatten.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode('overwrite')  \\\n",
    "    .option(\"mergeSchema\", True) \\\n",
    "    .save(delta_path)\n",
    "\n",
    "    \n",
    "spark.sql(\n",
    "        \"CREATE DATABASE IF NOT EXISTS {}\".format(database)\n",
    "    )\n",
    "\n",
    "spark.sql(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {}.{}\n",
    "        USING DELTA\n",
    "        LOCATION \"{}\"\n",
    "        \"\"\".format(\n",
    "            database, delta_table_name, delta_path\n",
    "        )\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|propertyDetails_a...|   bigint|       |\n",
      "|propertyDetails_a...|   string|       |\n",
      "|propertyDetails_a...|   string|       |\n",
      "|propertyDetails_c...|   bigint|       |\n",
      "|propertyDetails_c...|   string|       |\n",
      "|propertyDetails_c...|   bigint|       |\n",
      "|propertyDetails_c...|   bigint|       |\n",
      "|propertyDetails_d...|   string|       |\n",
      "|propertyDetails_g...|   bigint|       |\n",
      "|propertyDetails_h...|  boolean|       |\n",
      "|propertyDetails_h...|  boolean|       |\n",
      "|  propertyDetails_id|   bigint|       |\n",
      "|propertyDetails_i...|   string|       |\n",
      "|propertyDetails_i...|  boolean|       |\n",
      "|propertyDetails_i...|  boolean|       |\n",
      "|propertyDetails_i...|   string|       |\n",
      "|propertyDetails_i...|  boolean|       |\n",
      "|propertyDetails_i...|  boolean|       |\n",
      "|propertyDetails_i...|  boolean|       |\n",
      "|propertyDetails_l...|   string|       |\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED delta.`{}`\".format(delta_path)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      20|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM delta.`{}`\".format(delta_path)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
